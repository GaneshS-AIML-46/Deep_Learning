{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- CELL 1: imports & utils ---\n",
    "import os\n",
    "from pathlib import Path\n",
    "import math\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms, datasets, utils\n",
    "import torchvision\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# set deterministic seed for reproducibility\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "# utility to show image grid\n",
    "def show_tensor_images(img_tensor, title=None):\n",
    "    # img_tensor shape: (B, C, H, W) in range [0,1] or normalized\n",
    "    img_grid = utils.make_grid(\n",
    "        img_tensor.detach().cpu(), nrow=8, padding=2, normalize=True, scale_each=True\n",
    "    )\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.axis('off')\n",
    "    if title:\n",
    "        plt.title(title)\n",
    "    plt.imshow(img_grid.permute(1, 2, 0))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- CELL 2: configuration ---\n",
    "# IMPORTANT: change this to your local path that contains the CelebA images\n",
    "# For example: data_dir = r\"/home/user/datasets/celeba/img_align_celeba\"\n",
    "data_dir = r\"C:\\Users\\GANESH\\Downloads\\archive\\img_align_celeba\\img_align_celeba\" # <-- EDIT: path to your downloaded CelebA images\n",
    "\n",
    "\n",
    "# training hyperparameters\n",
    "img_size = 64 # resize images to 64x64 (typical for small VAE)\n",
    "batch_size = 128\n",
    "num_epochs = 1 # reduced to 1 epoch for a single output only\n",
    "learning_rate = 1e-3\n",
    "latent_dim = 128\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- CELL 3: dataset & dataloader ---\n",
    "# We assume images exist directly in data_dir, or in a subfolder. If they are in a CSV or different layout, adapt.\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((img_size, img_size)),\n",
    "    transforms.CenterCrop(img_size),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Check for common CelebA folder structure\n",
    "if os.path.isdir(os.path.join(data_dir, 'img_align_celeba')):\n",
    "    img_root = os.path.join(data_dir, 'img_align_celeba')\n",
    "else:\n",
    "    img_root = data_dir\n",
    "\n",
    "print('Using images from:', img_root)\n",
    "\n",
    "# Simple dataset loader\n",
    "from PIL import Image\n",
    "\n",
    "class SimpleImageDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, root, transform=None):\n",
    "        self.root = Path(root)\n",
    "        self.paths = [p for p in self.root.rglob('*') if p.suffix.lower() in {'.jpg', '.jpeg', '.png'}]\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        p = self.paths[idx]\n",
    "        img = Image.open(p).convert('RGB')\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img, 0\n",
    "\n",
    "# Initialize dataset and dataloader\n",
    "ds = SimpleImageDataset(img_root, transform=transform)\n",
    "dataloader = DataLoader(ds, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "\n",
    "print('Number of images in dataset:', len(dataloader.dataset))\n",
    "\n",
    "batch, _ = next(iter(dataloader))\n",
    "show_tensor_images(batch[:32], title='Sample training images')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- CELL 4: VAE model ---\n",
    ")\n",
    "self.fc_mu = nn.Linear(256*4*4, z_dim)\n",
    "self.fc_logvar = nn.Linear(256*4*4, z_dim)\n",
    "self.fc_dec = nn.Linear(z_dim, 256*4*4)\n",
    "self.decoder = nn.Sequential(\n",
    "nn.ConvTranspose2d(256, 128, 4, 2, 1),\n",
    "nn.BatchNorm2d(128),\n",
    "nn.ReLU(True),\n",
    "nn.ConvTranspose2d(128, 64, 4, 2, 1),\n",
    "nn.BatchNorm2d(64),\n",
    "nn.ReLU(True),\n",
    "nn.ConvTranspose2d(64, 32, 4, 2, 1),\n",
    "nn.BatchNorm2d(32),\n",
    "nn.ReLU(True),\n",
    "nn.ConvTranspose2d(32, image_channels, 4, 2, 1),\n",
    "nn.Sigmoid(),\n",
    ")\n",
    "\n",
    "\n",
    "def encode(self, x):\n",
    "h = self.encoder(x)\n",
    "h = h.view(h.size(0), -1)\n",
    "mu = self.fc_mu(h)\n",
    "logvar = self.fc_logvar(h)\n",
    "return mu, logvar\n",
    "\n",
    "\n",
    "def reparameterize(self, mu, logvar):\n",
    "std = torch.exp(0.5 * logvar)\n",
    "eps = torch.randn_like(std)\n",
    "return mu + eps * std\n",
    "\n",
    "\n",
    "def decode(self, z):\n",
    "h = self.fc_dec(z)\n",
    "h = h.view(h.size(0), 256, 4, 4)\n",
    "x = self.decoder(h)\n",
    "return x\n",
    "\n",
    "\n",
    "def forward(self, x):\n",
    "mu, logvar = self.encode(x)\n",
    "z = self.reparameterize(mu, logvar)\n",
    "x_recon = self.decode(z)\n",
    "return x_recon, mu, logvar\n",
    "\n",
    "\n",
    "model = ConvVAE(image_channels=3, hidden_dim=256, z_dim=latent_dim).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "bce_loss = nn.BCELoss(reduction='sum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- CELL 5: training loop ---\n",
    "model.train()\n",
    "save_dir = Path('./vae_outputs')\n",
    "save_dir.mkdir(exist_ok=True)\n",
    "\n",
    "\n",
    "for epoch in range(1, num_epochs+1):\n",
    "train_loss = 0.0\n",
    "pbar = tqdm(enumerate(dataloader), total=len(dataloader), desc=f'Epoch {epoch}/{num_epochs}')\n",
    "for i, (imgs, _) in pbar:\n",
    "imgs = imgs.to(device)\n",
    "optimizer.zero_grad()\n",
    "recon, mu, logvar = model(imgs)\n",
    "recon_loss = bce_loss(recon, imgs)\n",
    "kl = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "loss = recon_loss + kl\n",
    "loss.backward()\n",
    "optimizer.step()\n",
    "train_loss += loss.item()\n",
    "pbar.set_postfix({'avg_loss': train_loss/((i+1)*batch_size)})\n",
    "avg_loss = train_loss / len(dataloader.dataset)\n",
    "print(f'Epoch {epoch} Average loss: {avg_loss:.4f}')\n",
    "\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "z = torch.randn(64, latent_dim).to(device)\n",
    "samples = model.decode(z)\n",
    "show_tensor_images(samples, title=f'Generated samples (epoch {epoch})')\n",
    "grid = utils.make_grid(samples.detach().cpu(), nrow=8, normalize=True)\n",
    "torchvision.utils.save_image(grid, save_dir / f'generated_epoch_{epoch}.png')\n",
    "model.train()\n",
    "\n",
    "\n",
    "print('Training finished. Saved outputs in:', save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
